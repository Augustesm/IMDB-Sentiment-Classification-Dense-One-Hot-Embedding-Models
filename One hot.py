# -*- coding: utf-8 -*-
"""2.1 ONE-HOT (10-31)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aEqjZxGaooreuoIhuzJGlReiKbTpkfMB
"""

from keras.datasets import imdb
import keras
from keras import models
from keras import layers
from tensorflow.keras.layers import Dense, Input
import numpy as np
from keras import optimizers
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
num_words=10000)

n=1
print(train_data[n])

word_index = imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[1]])
print(decoded_review)

#onehot kodavimas
#zodynas turi 10 000 zodziu, one-hot turi reiksmes 1 arba 0, ignoruoja zodziu tvarka,
def vectorize_sequences(sequences, dimension=10000):
  #vienas sakinys = viena eilute
    results = np.zeros((len(sequences), dimension))
    print((len(sequences)))
    for i, sequence in enumerate(sequences):
         results[i, sequence] = 1.
    return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

sequences=train_data[:10]
for sequence_number, words in enumerate(sequences):
    print(f"Sequence {sequence_number + 1}: {words}")

x_train = vectorize_sequences(train_data)

np.set_printoptions(threshold=np.inf)
print(x_train[2][:100])
print(type(x_train[2]))

model = models.Sequential()
model.add(Input(shape=(10000,))) #one-hot vektorius
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) #isejimas 0, 1 binarinei klasifikacijai

model.compile(optimizer=optimizers.Adam(learning_rate=0.001),
loss='binary_crossentropy', #kiek blogai atspeja tikimybes
metrics=['accuracy'])
model.summary()

history = model.fit(x_train,
                    train_labels,
                    epochs=20,
                    batch_size=32,
                    validation_split=0.4)

#kaip sekasi mokytis

plt.plot(history.history['loss'],
         label='Loos mokymo imtyje')
plt.plot(history.history['val_loss'],
         label='Loss validavimo imtyje')
plt.xlabel('Apmokymas')
plt.ylabel('Loss')
plt.legend()
plt.show()

scores = model.evaluate(x_test, test_labels, verbose=1)
#blogas
#input_text="It's hard to overstate how bad this is. A stellar comedic cast and any one of them could have created a better movie if it was just 90 minutes of them reading the phone book. Do not ever watch this!"
#geras
input_text="I've made about 20 films and 5 of them are pretty good-Tom Hanks. Forrest Gump is one of the best movies of all time, guaranteed. I really just love this movie and it has such a special place in my heart. The performances are just so unforgettable and never get out of your head. The characters, I mean the actors turned into them and that's what got to me. The lines are so memorable, touching, and sometimes hilarious."
#nput_text="My expectations for John Wick: Chapter 4 was high, these films have become some of my favorite action films of all time, and I can say this without giving it a second thought how it met those expectations. The thugs and easy obstacles are nothing new, except for how they too have ballistic suits like our protagonist. But this Chapter begins with a recovered Baba Yaga who's out for blood. Actors and returning characters along with Keanu Reeves as John Wick is Laurence Fishburne as the Bowery King, Ian McShane as Winston Scott, and Lance Reddick (in one of his final roles) as Charon. New roles which I loved was Donnie Yen as Caine: a blind High Table assassin and an old friend of John Wick."

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Sukuriame Tokenizer objektą ir priskiriame jam imdb.word_index
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>',filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n -', lower=True)
tokenizer.word_index = word_index # taikomas imdb žodynas sakinio kodavimui
encoded_text = tokenizer.texts_to_sequences([input_text])
encoded_text = [[1] + [i+3 if i is not None else 2 for i in lst] for lst in encoded_text] #IMDB kodavimas
print(encoded_text[0])

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

xv = vectorize_sequences(encoded_text)

prediction=model.predict(xv)
print(prediction)

for word, index in word_index.items():
    if word == 'hard':
        print(f"The ID of '{word}' in IMDb's word_index is {index+3}.")

prediction=model.predict(xv)
print(prediction)