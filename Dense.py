# -*- coding: utf-8 -*-
"""2.1 DENSE (10-31)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dwl3NSNMiJBfUpNwccQSTL_A_vwbZbEw
"""

from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Flatten
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

max_words=10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)

#Tai 50 000 filmų recenzijų iš IMDB duomenų rinkinys, pažymėtas pagal nuotaikas (teigiamas/neigiamas).
#Atsiliepimai buvo iš anksto apdoroti, o kiekvienas atsiliepimas užkoduotas kaip žodžių indeksų sąrašas (sveikieji skaičiai).
#Kiekvienam unikaliam mokymo rinkinio žodžiui sukuriamas unikalus indeksas. Šis indeksavimas atliekamas ta tvarka, kuria žodžiai atsiranda tekstuose.
#Patogumo dėlei žodžiai indeksuojami pagal bendrą dažnumą duomenų rinkinyje, todėl, pavyzdžiui, sveikasis skaičius "3" koduoja trečią pagal dažnumą žodį duomenyse.
#Tai leidžia greitai atlikti filtravimo operacijas, pvz: "atsižvelgti tik į 10 000 dažniausiai pasitaikančių žodžių, bet pašalinti 20 mažniausiai pasitaikančių žodžių.

print('Mokymui skirta', x_train.shape, 'duomenų.')
print('Testavimui skirta', x_test.shape, 'duomenų.')
print(type(x_train))

n=1 # pranešimo numeris
print('Pranešimas:', x_train[n]) # žodynas+3
print('Pranešimo ilgis:', len(x_train[n]))
print('Pranešimo žyma:', y_train[n])

# 0 - tarpas
# 1 - pradžia
# 2 - nežinomas žodis, tas kuris neįėjo į 10000
#code-3 atimame tas tris reikšmes
#Žodžiai numeruojami nuo indeksto 1, tačiau koduojant tas numeravimas pasislenka 3 simboliais, norint atstatyti numeravimą būtina atimti 3

#žodynas, kuris naudojamas žodžių kodavime// raktas žodis - reikšmė dažnis
word_index=imdb.get_word_index()

print(word_index)
print(type(word_index))
print(len(word_index))

word_index_max = max(word_index.values())
word_index_min = min(word_index.values())
print("Minimalus indeksas žodyne:", word_index_min, ";", "maksimalus indeksas žodyne:", word_index_max, ".")

# Rūšiuoja pagal indeksą (pagal antrą elementą)
sorted_words = sorted(word_index.items(), key=lambda x: x[1], reverse=False) # didėjimo tvarka, reverse=True-mažėjimo tvarka

# Išveskite pirmus N žodžių ir jų indeksą
N = 20  # Keiskite N į norimą skaičių
for i in range(N):
    word, index = sorted_words[i]
    print(f"{word}: {index}")

# mums reikia pakeisti raktą su reikšme ir išvesti žodžius pagal dažnį
reverse_word_index=dict()
for id, zodis in word_index.items(): # pereiname per visus elementus
  reverse_word_index[zodis]=id # sukursime žodyną, kuriame id - skaičius, žodis- tekstas

# dažniausiai naudojami žodžiai komentaruose
for i in range(1,11):
  print(i, '=', reverse_word_index[i])

# dažniausiai naudojami žodžiai komentaruose
for i in range(190,200):
  print(i, '=', reverse_word_index[i])

# dažniausiai naudojami žodžiai komentaruose
for i in range(9985,10000):
  print(i, '=', reverse_word_index[i])

  # dažniausiai naudojami žodžiai komentaruose
for i in range(11000,11005):
  print(i, '=', reverse_word_index[i])


Modelio_index = 13 # kodas, kuris bus naudojamas modeliui apmokyti, IMDb's dataset (1 sakinio pradžia, 2 - nežinomas žodis)
Dicindex=Modelio_index-3 #teksto kodavimas [žodynas]
word = next(word for word, index_in_dict in word_index.items() if index_in_dict == Dicindex)

print(f"Modelio kodo indeksas: {Modelio_index}, žodyno indeksas: {Dicindex}, žodis: {word}")

index=n
message=''
for code in x_train[index]:
  word=reverse_word_index.get(code-3,'??') ## ?? - žodis kurio nėra žodyne
  message += word + ' '
message

maxlen = 200 # suvienodina komentaro ilgį
x_train = pad_sequences(x_train, maxlen=maxlen) # jeigu komentaras per ilgas - pašalina žodžius, jei per mažas - prideda tarpus (skaičius 0)
x_test = pad_sequences(x_test, maxlen=maxlen)

print('Pranešimas:', x_train[n])

for word, index in word_index.items():
    if word == 'hair':
        print(f"The ID of '{word}' in IMDb's word_index is {index+3}.")

model = Sequential()
model.add(Input(shape=(maxlen,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(64, activation='relu')) # pilnai sujungtas sluoksnis - dense sluoksniai ismoksta pozymius
model.add(Dense(64, activation='relu')) # pilnai sujungtas sluoksnis
model.add(Dense(1, activation='sigmoid')) # skirtas teksto klasifikacijai, atsakymai nuo 0 iki 1, atspausdina tikimybe ar neigiamas ar teigiamas

model.compile(optimizer='adam', #pritaiko mokymosi greiti automatiskai, greitas ir stabilus algoritmas
              loss='binary_crossentropy',  # binarinė klasifikacija
              metrics=['accuracy'])

model.summary()

history = model.fit(x_train,
                    y_train,
                    epochs=30,
                    batch_size=32,
                    validation_split=0.2)

import matplotlib.pyplot as plt
plt.plot(history.history['loss'],
         label='Loos mokymo imtyje')
plt.plot(history.history['val_loss'],
         label='Loss validavimo imtyje')
plt.xlabel('Apmokymas')
plt.ylabel('Loss')
plt.legend()
plt.show()

scores = model.evaluate(x_test, y_test, verbose=1)

input_text="As the title suggests, the two begin their relationship by seemingly loathing each other. They clash over issues of taste, given that Lucy started out in an upmarket literary fiction publisher and Joshua worked for a more mass-market house that published trashy supermarket-bound titles. The merger of their two firms results in the two sharing an office – desks facing each other across a gulf of class, opposing values and a rug – as they minister to the needs of co-CEOs Sakina Jaffrey and Corbin Bernsen." #  2 balai, https://www.theguardian.com/film/2022/aug/08/the-hating-game-review-sugary-manhattan-romcom-that-goes-down-easily

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np


# Sukuriame Tokenizer objektą ir priskiriame jam imdb.word_index
# OOV -out of vocabulary
#teksta pavercia i skaiciu sekas pagal zodyna
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>',filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n -', lower=True)
tokenizer.word_index = word_index # taikomas imdb žodynas sakinio kodavimui
encoded_text = tokenizer.texts_to_sequences([input_text])

maxlen = 200

#Pirmasis elementas [1] yra pridėtas į pradžią kaip naujas indeksas, skirtas komentaro pradžiai.
#Tada, kiekvienam kodui i lst sąraše, yra atliekami tokie veiksmai:
#Jei i nėra lygus None, tai prie jo pridedama 3 ir gaunamas naujas kodas.
#Šis veiksmas reikalingas dėl to, kad pirmi trys kodai buvo naudojami Tokenizer bibliotekoje kaip rezervuoti kodai - "<UNUSED>", <START> ir <UNK>.
#Jei i yra lygus None, tai prie jo pridedama 2 ir gaunamas naujas kodas, kuris yra skirtas nežinomam žodžiui.

# Koduojame komentarą
encoded_text = [[1] + [i+3 if i is not None else 2 for i in lst] for lst in encoded_text]
x=encoded_text
print(encoded_text)

# Dekoduojame komentarą
decoded_comment = ' '.join([reverse_word_index.get(i-3, '??') for i in x[0] if i > 3])

# Taikome padding nustatytam seka ilgiui - tam kad komentarai butu vienodo ilgio
padded = pad_sequences(encoded_text, maxlen=maxlen, padding='pre') # Pridedamas ["<PAD>"] = 0
print('Pakoreguota seka:', padded)

# Spausdiname dekoduotą komentarą
print('Dekoduotas komentaras:', decoded_comment)

prediction=model.predict(padded)
print(prediction)