# -*- coding: utf-8 -*-
"""2.1 EMBEDDING (10-31)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m6-dDtDugO_g4pc0qrVZaqg44i-m5CcO
"""

from keras.datasets import imdb
import keras
from keras import models
from keras import layers
from tensorflow.keras.layers import Dense, Embedding, Flatten, Input
import numpy as np
from keras import optimizers
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
num_words=10000)

print(train_data[1])

word_index = imdb.get_word_index()
reverse_word_index = dict(
[(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join(
[reverse_word_index.get(i - 3, '?') for i in train_data[1]])
print(decoded_review)

sequences=train_data[:10]
for sequence_number, words in enumerate(sequences):
    print(f"Sequence {sequence_number + 1}: {words}")

maxlen = 200 # suvienodina komentaro ilgį
x_train = pad_sequences(train_data, maxlen=maxlen) # jeigu komentaras per ilgas - pašalina žodžius, jei per mažas - prideda tarpus (skaičius 0)
x_test = pad_sequences(test_data, maxlen=maxlen)

model = models.Sequential()
model.add(Input(shape=(maxlen,))) skaiciu vektoriu, jis moka zodzius su panasia reiksme atskirti tada vektoriai bus panasus
#zodziu indeksai paverciami vektoriais, kiekvienas zodis turi
model.add(Embedding(10000, 8)) #Maksimalaus įėjimo ilgio apibrėžimas (objektai-pvz., maksimalus ilgis, 8) , input_length=maxlen
model.add(Flatten()) #Trimačio tenzoriau pertvarkymas į dvimatį tenzorių (objektai, maksimalus ilgis*8)
model.add(layers.Dense(16, activation='relu')) #kiekviena neurona jungia su ankstesniais, randa daznus zodziu derinius, padeda optimizuoti sprendimo riba
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) #atspausdina tikimybe, kas tinka binarinei klasifickacija

model.compile(optimizer=optimizers.Adam(learning_rate=0.001),
loss='binary_crossentropy', #praradimo funkcija, jei modelis labai tiksliai pataike - maza loss
metrics=['accuracy'])
model.summary()

history = model.fit(x_train,
                    train_labels,
                    epochs=20,
                    batch_size=32,
                    validation_split=0.2)

plt.plot(history.history['loss'],
         label='Loos mokymo imtyje')
plt.plot(history.history['val_loss'],
         label='Loss validavimo imtyje')
plt.xlabel('Apmokymas')
plt.ylabel('Loss')
plt.legend()
plt.show()

scores = model.evaluate(x_test, test_labels, verbose=1)

#ivertinam komentaram, kurio modelis dar nemate
#blogas
input_text="It's hard to overstate how bad this is. A stellar comedic cast and any one of them could have created a better movie if it was just 90 minutes of them reading the phone book. Do not ever watch this!"
#geras
#input_text="I've made about 20 films and 5 of them are pretty good-Tom Hanks. Forrest Gump is one of the best movies of all time, guaranteed. I really just love this movie and it has such a special place in my heart. The performances are just so unforgettable and never get out of your head. The characters, I mean the actors turned into them and that's what got to me. The lines are so memorable, touching, and sometimes hilarious."
#nput_text="My expectations for John Wick: Chapter 4 was high, these films have become some of my favorite action films of all time, and I can say this without giving it a second thought how it met those expectations. The thugs and easy obstacles are nothing new, except for how they too have ballistic suits like our protagonist. But this Chapter begins with a recovered Baba Yaga who's out for blood. Actors and returning characters along with Keanu Reeves as John Wick is Laurence Fishburne as the Bowery King, Ian McShane as Winston Scott, and Lance Reddick (in one of his final roles) as Charon. New roles which I loved was Donnie Yen as Caine: a blind High Table assassin and an old friend of John Wick."

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Sukuriame Tokenizer objektą ir priskiriame jam imdb.word_index
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>',filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n -', lower=True)
tokenizer.word_index = word_index # taikomas imdb žodynas sakinio kodavimui
encoded_text = tokenizer.texts_to_sequences([input_text])
encoded_text = [[1] + [i+3 if i is not None else 2 for i in lst] for lst in encoded_text] #IMDB kodavimas
x_padded = pad_sequences(encoded_text, maxlen=200)

prediction=model.predict(x_padded)
print(np.round(prediction,3))